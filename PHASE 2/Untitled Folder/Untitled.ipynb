{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Skip to content\n",
    "Search or jump to…\n",
    "\n",
    "Pull requests\n",
    "Issues\n",
    "Marketplace\n",
    "Explore\n",
    " \n",
    "@Harshapriya123 \n",
    "google\n",
    "/\n",
    "next-prediction\n",
    "18\n",
    "264\n",
    "74\n",
    "Code\n",
    "Issues\n",
    "Pull requests\n",
    "Actions\n",
    "Projects\n",
    "Wiki\n",
    "Security\n",
    "Insights\n",
    "next-prediction/code/preprocess.py /\n",
    "@JunweiLiang\n",
    "JunweiLiang Tested with Py3\n",
    "Latest commit 4bf6ec2 on Nov 5, 2020\n",
    " History\n",
    " 1 contributor\n",
    "Executable File  839 lines (696 sloc)  29.8 KB\n",
    "  \n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\"\"\"Preprocess annotations for training and testing.\n",
    "See README for running instructions and\n",
    "download_*.sh for downloading annotations.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import argparse\n",
    "#import cPickle as pickle\n",
    "import pickle\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"traj_path\", help=\"Path to the processed trajectory files\")\n",
    "parser.add_argument(\"output_path\", help=\"Path to put the preprocessed files\")\n",
    "parser.add_argument(\"--obs_len\", default=8, type=int)\n",
    "parser.add_argument(\"--pred_len\", default=12, type=int)\n",
    "parser.add_argument(\"--min_ped\", default=0, type=int,\n",
    "                    help=\"Minimal pedestrian in the a frame \"\n",
    "                         \"to be considered valid datapoint\"\n",
    "                         \". Set 1 for ETH/UCY experiment \"\n",
    "                         \"to be consistent with Social GAN.\")\n",
    "\n",
    "# Input features/annotations\n",
    "parser.add_argument(\"--add_kp\", action=\"store_true\")\n",
    "parser.add_argument(\"--kp_path\", default=None)\n",
    "\n",
    "parser.add_argument(\"--add_person_box\", action=\"store_true\")\n",
    "parser.add_argument(\"--person_box_path\", default=None)\n",
    "parser.add_argument(\"--person_boxkey2id_p\", default=None,\n",
    "                    help=\"For reproducing experiments,\"\n",
    "                    \" need person_boxkey2id from previous\"\n",
    "                    \" preprocessed files to get the same \"\n",
    "                    \"box id so you can you the saved person feature.\")\n",
    "\n",
    "parser.add_argument(\"--add_other_box\", action=\"store_true\")\n",
    "parser.add_argument(\"--other_box_path\", default=None)\n",
    "\n",
    "parser.add_argument(\"--add_activity\", action=\"store_true\")\n",
    "parser.add_argument(\"--activity_path\", default=None)\n",
    "\n",
    "parser.add_argument(\"--add_scene\", action=\"store_true\")\n",
    "parser.add_argument(\"--scene_feat_path\", default=None)\n",
    "parser.add_argument(\"--scene_map_path\", default=None)\n",
    "parser.add_argument(\"--scene_id2name\", default=None)\n",
    "parser.add_argument(\"--scene_h\", type=int, default=36)\n",
    "parser.add_argument(\"--scene_w\", type=int, default=64)\n",
    "\n",
    "parser.add_argument(\"--add_grid\", action=\"store_true\")\n",
    "parser.add_argument(\"--video_h\", type=int, default=1080)\n",
    "parser.add_argument(\"--video_w\", type=int, default=1920)\n",
    "\n",
    "# Specially for ETH/UCY benchmark\n",
    "parser.add_argument(\"--traj_pixel_lst\", default=None,\n",
    "                    help=\"For ETH/UCY benchmark, \"\n",
    "                         \"need to use x,y in pixel to get grid location\")\n",
    "parser.add_argument(\"--feature_no_split\", action=\"store_true\",\n",
    "                    help=\"There is not train/val/test\"\n",
    "                         \" folder in the feature directory.\")\n",
    "parser.add_argument(\"--reverse_xy\", action=\"store_true\",\n",
    "                    help=\"The trajectory file is in frameidx\"\n",
    "                         \", personidx, y, x.\")\n",
    "\n",
    "\n",
    "def main(args):\n",
    "  # Compute the scene grid\n",
    "  if args.add_grid:\n",
    "    args.scene_grid_strides = (2, 4)\n",
    "    args.num_scene_grid = len(args.scene_grid_strides)\n",
    "\n",
    "    args.scene_grids = []\n",
    "    # the following is consistent with tensorflow conv2d when given odd input\n",
    "    for stride in args.scene_grid_strides:\n",
    "      h, w = args.scene_h, args.scene_w\n",
    "      this_h, this_w = round(h*1.0/stride), round(w*1.0/stride)\n",
    "      this_h, this_w = int(this_h), int(this_w)\n",
    "      args.scene_grids.append((this_h, this_w))\n",
    "\n",
    "    # Get the center point for each scale's each grid\n",
    "    args.scene_grid_centers = []\n",
    "    for h, w in args.scene_grids:\n",
    "      h_gap, w_gap = args.video_h*1.0/h, args.video_w*1.0/w\n",
    "      centers_x = np.cumsum([w_gap for _ in range(w)]) - w_gap/2.0\n",
    "      centers_y = np.cumsum([h_gap for _ in range(h)]) - h_gap/2.0\n",
    "      centers_xx = np.tile(np.expand_dims(centers_x, axis=0), [h, 1])\n",
    "      centers_yy = np.tile(np.expand_dims(centers_y, axis=1), [1, w])\n",
    "      centers = np.stack((centers_xx, centers_yy), axis=-1)  # [H,W,2]\n",
    "      args.scene_grid_centers.append(centers)\n",
    "\n",
    "  # load alternative xy in pixels for ETH/UCY benchmark experiments\n",
    "  args.traj_pixel = None\n",
    "  if args.traj_pixel_lst is not None:\n",
    "    args.traj_pixel = {}\n",
    "    delim = \"\\t\"\n",
    "    with open(args.traj_pixel_lst, \"r\") as traj_pixel_lst:\n",
    "      for pixel_file in traj_pixel_lst:\n",
    "        pixel_file = pixel_file.strip()\n",
    "        filename = os.path.splitext(os.path.basename(pixel_file))[0]\n",
    "        args.traj_pixel[filename] = {}\n",
    "        for line in open(pixel_file):\n",
    "          fid, pid, x, y = line.strip().split(delim)\n",
    "          p_key = \"%d_%d\" % (float(fid), float(pid))\n",
    "          x = float(x)\n",
    "          y = float(y)\n",
    "          assert float(x) <= args.video_w, line\n",
    "          assert float(y) <= args.video_h, line\n",
    "          args.traj_pixel[filename][p_key] = [float(x), float(y)]\n",
    "\n",
    "  args.seq_len = args.obs_len + args.pred_len\n",
    "\n",
    "  if not os.path.exists(args.output_path):\n",
    "    os.makedirs(args.output_path)\n",
    "\n",
    "  # For creating the same boxid as previous experiment\n",
    "  args.person_boxkey2id = None\n",
    "  if args.person_boxkey2id_p is not None:\n",
    "    with open(args.person_boxkey2id_p, \"rb\") as f:\n",
    "      args.person_boxkey2id = pickle.load(f)\n",
    "\n",
    "  prepro_each(args.traj_path, \"train\", os.path.join(\n",
    "      args.output_path, \"data_train.npz\"), args)\n",
    "  prepro_each(args.traj_path, \"val\", os.path.join(\n",
    "      args.output_path, \"data_val.npz\"), args)\n",
    "  prepro_each(args.traj_path, \"test\", os.path.join(\n",
    "      args.output_path, \"data_test.npz\"), args)\n",
    "\n",
    "\n",
    "def prepro_each(traj_path, split, prepro_path, args):\n",
    "  \"\"\"Preprocess each data split into one npz file.\n",
    "  Args:\n",
    "    traj_path: path to the trajectory annotation files\n",
    "    split: train/val/test\n",
    "    prepro_path: path to the output npz file\n",
    "    args: arguments\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  traj_path = os.path.join(traj_path, split)\n",
    "\n",
    "  # traj_path each file is a video, with frameid, personid, x, y\n",
    "  videos = glob.glob(os.path.join(traj_path, \"*.txt\"))\n",
    "\n",
    "  delim = \"\\t\"\n",
    "\n",
    "  seq_len = args.seq_len\n",
    "  obs_len = args.obs_len\n",
    "\n",
    "  # collect data for all videos\n",
    "  seq_list = []  # [N, seq_len, 2], N is frames*person_per_frame\n",
    "  seq_list_rel = []\n",
    "  num_person_in_start_frame = []\n",
    "\n",
    "  # so we could refer to the original frame for each time step\n",
    "  seq_frameidx_list = []  # [N, seq_len]\n",
    "  seq_vid_list = []  # [N] ,N videoid, int\n",
    "  vid2name = {}\n",
    "  total_frame_used = {}\n",
    "\n",
    "  seq_grid_class_list = []  # [N, strides, seq_len]\n",
    "  seq_grid_target_list = []  # [N, strides, seq_len, 2]\n",
    "\n",
    "  # the person traj's boxes\n",
    "  box_seq_list = []  # [N, seq_len, 4]\n",
    "\n",
    "  person_boxid_list = []  # [N,seq_len]\n",
    "  person_boxid2key = {}  # frameIdx_personId ->\n",
    "  person_boxkey2id = {}\n",
    "\n",
    "  # the other boxes in the last observed frame\n",
    "  # [N,1] a list of variable number of boxes\n",
    "  other_box_seq_list = []\n",
    "  # [N,1] # a list of variable number of boxes classes\n",
    "  other_box_class_seq_list = []\n",
    "\n",
    "  # activity annotation, currently just use the last observed frame\n",
    "  # for current activity and the last predict frame for future activity\n",
    "  cur_act_list = []  # [N,1] # a list of act id\n",
    "  future_act_list = []  # [N,1] # a list of act id, could be empty?\n",
    "\n",
    "  kp_num = 17  # coco style\n",
    "  kp_list = []  # [N, seq_len, 17, 2]\n",
    "  kp_list_rel = []\n",
    "\n",
    "  scene_list = []  # [N, seq_len, 1] # only the frame feature id\n",
    "  # will have a final scene feature of [num_frame, H, W, class]\n",
    "  # scene class mask\n",
    "  # save only the unique frame\n",
    "  scene_feat_dict = {}  # #frame to feature\n",
    "  scene_key2feati = {}\n",
    "  scene_h, scene_w = args.scene_h, args.scene_w\n",
    "\n",
    "  # load the classes that we used for scene segmantics\n",
    "  if args.add_scene:\n",
    "    with open(args.scene_id2name, \"r\") as f:\n",
    "      scene_id2name = json.load(f)  # {\"oldid2new\":,\"id2name\":}\n",
    "    scene_oldid2new = scene_id2name[\"oldid2new\"]\n",
    "    scene_oldid2new = {\n",
    "        int(oldi): scene_oldid2new[oldi] for oldi in scene_oldid2new}\n",
    "    # for background class or other class that we ignored\n",
    "    #assert not scene_oldid2new.has_key(0)\n",
    "    assert 0 not in scene_oldid2new\n",
    "    scene_oldid2new[0] = 0\n",
    "    total_scene_class = len(scene_oldid2new)\n",
    "    scene_id2name = scene_id2name[\"id2name\"]\n",
    "    scene_id2name[0] = \"BG\"\n",
    "    assert len(scene_oldid2new) == len(scene_id2name)\n",
    "\n",
    "  # person trajectory processing part is modified from Social GAN\n",
    "  # https://github.com/agrimgupta92/sgan/blob/master/sgan/data/trajectories.py\n",
    "  # to keep the experimental setting the same\n",
    "  for video in tqdm(videos, ascii=True):\n",
    "    videoname = os.path.splitext(os.path.basename(video))[0]\n",
    "    vid = len(vid2name)\n",
    "    vid2name[vid] = videoname\n",
    "\n",
    "    # load other features if necessary\n",
    "    kp_feats = {}  # \"frameidx_personId\"\n",
    "    # \"frameid\" -> scene_feat_file_path # load it dynamically\n",
    "    scene_frameid2file = {}\n",
    "    if args.add_kp:\n",
    "      kp_file_path = os.path.join(args.kp_path, split, \"%s.p\" % videoname)\n",
    "      with open(kp_file_path, \"rb\") as f:\n",
    "\n",
    "        if sys.version_info.major == 2:\n",
    "          # this works for py2 since the pickle is generated with py2 code\n",
    "          kp_feats = pickle.load(f)\n",
    "        else:\n",
    "          # ugly so it is py3 compatitable\n",
    "          kp_feats = pickle.load(f, encoding=\"bytes\")\n",
    "          new_kp_feats = {}\n",
    "          for k in kp_feats:\n",
    "            new_kp_feats[k.decode(\"utf-8\")] = kp_feats[k]\n",
    "          kp_feats = new_kp_feats\n",
    "\n",
    "    if args.add_scene:\n",
    "      # get the frameid to file name since scene is not extracted every frames\n",
    "      scene_file = os.path.join(args.scene_map_path, split, \"%s.p\" % videoname)\n",
    "      if args.feature_no_split:\n",
    "        scene_file = os.path.join(args.scene_map_path, \"%s.p\" % videoname)\n",
    "      with open(scene_file, \"rb\") as f:\n",
    "        scene_frameid2file = pickle.load(f)\n",
    "      for frameid in scene_frameid2file:\n",
    "        scene_frameid2file[frameid] = os.path.join(\n",
    "            args.scene_feat_path, scene_frameid2file[frameid])\n",
    "\n",
    "    if args.add_person_box:\n",
    "      person_box_path = os.path.join(\n",
    "          args.person_box_path, split, \"%s.p\" % videoname)\n",
    "      if args.feature_no_split:\n",
    "        person_box_path = os.path.join(\n",
    "            args.person_box_path, \"%s.p\" % videoname)\n",
    "      with open(person_box_path, \"rb\") as f:\n",
    "        person_boxes = pickle.load(f)\n",
    "\n",
    "    if args.add_other_box:\n",
    "      other_box_path = os.path.join(\n",
    "          args.other_box_path, split, \"%s.p\" % videoname)\n",
    "      if args.feature_no_split:\n",
    "        other_box_path = os.path.join(args.other_box_path, \"%s.p\" % videoname)\n",
    "      with open(other_box_path, \"rb\") as f:\n",
    "        other_boxes = pickle.load(f)\n",
    "\n",
    "    if args.add_activity:\n",
    "      activity_path = os.path.join(\n",
    "          args.activity_path, split, \"%s.p\" % videoname)\n",
    "      with open(activity_path, \"rb\") as f:\n",
    "        activities = pickle.load(f)\n",
    "\n",
    "    # [N,4], [frame_idx, person_id,x,y]\n",
    "    data = []\n",
    "    with open(video, \"r\") as traj_file:\n",
    "      for line in traj_file:\n",
    "        if args.reverse_xy:\n",
    "          fidx, pid, y, x = line.strip().split(delim)\n",
    "        else:\n",
    "          fidx, pid, x, y = line.strip().split(delim)\n",
    "        data.append([fidx, pid, x, y])\n",
    "    data = np.array(data, dtype=\"float32\")\n",
    "\n",
    "    # assuming the frameIdx is sorted in ASC\n",
    "    frames = np.unique(data[:, 0]).tolist()  # all frame_idx\n",
    "    frame_data = []  # [num_frame, K,4]\n",
    "    for frame in frames:\n",
    "      frame_data.append(data[frame == data[:, 0], :])\n",
    "\n",
    "    for idx, frame in enumerate(frames):\n",
    "      # [N, 4] # N is seq_len* person_per_frame\n",
    "      # [obs_frames -> pre_frames all data]\n",
    "      cur_seq_data = np.concatenate(frame_data[idx:idx + seq_len], axis=0)\n",
    "      # [K] # all person Id in this sequence frames [20 frames]\n",
    "      persons_in_cur_seq = np.unique(cur_seq_data[:, 1])\n",
    "      num_person_in_cur_seq = len(persons_in_cur_seq)\n",
    "      # [K, seq_len, 2] # x,y for all person sequence, starting at idx frame\n",
    "      cur_seq = np.zeros((num_person_in_cur_seq, seq_len, 2), dtype=\"float32\")\n",
    "      # relative x,y for training\n",
    "      cur_seq_rel = np.zeros((num_person_in_cur_seq, seq_len, 2),\n",
    "                             dtype=\"float32\")\n",
    "\n",
    "      # frameid for each seq timestep\n",
    "      cur_seq_frame = np.zeros((num_person_in_cur_seq, seq_len), dtype=\"int32\")\n",
    "      cur_seq_vid = np.zeros((num_person_in_cur_seq), dtype=\"int32\")\n",
    "      cur_seq_vid[:] = vid  # all this sequence is in the same video obviously\n",
    "\n",
    "      # for grid classification and target\n",
    "      if args.add_grid:\n",
    "        cur_seq_grids_class = np.zeros(\n",
    "            (num_person_in_cur_seq, args.num_scene_grid, seq_len),\n",
    "            dtype=\"int32\")\n",
    "        cur_seq_grids_target = np.zeros(\n",
    "            (num_person_in_cur_seq, args.num_scene_grid, seq_len, 2),\n",
    "            dtype=\"float32\")\n",
    "\n",
    "      count_person = 0\n",
    "\n",
    "      if args.add_kp:\n",
    "        # absolute pixexl\n",
    "        kp_feat = np.zeros((num_person_in_cur_seq, seq_len, kp_num, 2),\n",
    "                           dtype=\"float32\")\n",
    "        # velocity\n",
    "        kp_feat_rel = np.zeros((num_person_in_cur_seq, seq_len, kp_num, 2),\n",
    "                               dtype=\"float32\")\n",
    "\n",
    "      if args.add_person_box:\n",
    "        person_box = np.zeros((num_person_in_cur_seq, seq_len, 4),\n",
    "                              dtype=\"float32\")\n",
    "        person_boxids = np.zeros((num_person_in_cur_seq, seq_len),\n",
    "                                 dtype=\"int32\")\n",
    "\n",
    "      if args.add_other_box:\n",
    "        other_box = []\n",
    "        other_box_class = []\n",
    "\n",
    "      if args.add_activity:\n",
    "        cur_activity = []\n",
    "        future_activity = []\n",
    "\n",
    "      if args.add_scene:\n",
    "        scene_featidx = np.zeros((num_person_in_cur_seq, seq_len, 1),\n",
    "                                 dtype=\"int\")\n",
    "        # this frame to the rest frame for all the persons should be the same\n",
    "        frame_idxs = frames[idx:idx+seq_len]\n",
    "\n",
    "        for i, frame_idx in enumerate(frame_idxs):\n",
    "          # key = \"%s_%d\"%(videoname,frame_idx)\n",
    "          # so we only load unique feat once\n",
    "          key = scene_frameid2file[frame_idx]\n",
    "\n",
    "          if key not in scene_key2feati:\n",
    "            feati = len(scene_feat_dict.keys())\n",
    "            # get the feature new i\n",
    "            # (H,W)\n",
    "            scene_feat_dict[key] = np.load(key)\n",
    "            scene_key2feati[key] = feati\n",
    "\n",
    "          else:\n",
    "            feati = scene_key2feati[key]\n",
    "\n",
    "          scene_featidx[:, i, :] = feati\n",
    "\n",
    "      for person_id in persons_in_cur_seq:\n",
    "        # traverse all person starting from idx frames for 20 frames\n",
    "\n",
    "        # [M, 4]\n",
    "        cur_person_seq = cur_seq_data[cur_seq_data[:, 1] == person_id, :]\n",
    "\n",
    "        if len(cur_person_seq) != seq_len:\n",
    "          # skipping the sequence not fully cover in this frames\n",
    "          continue\n",
    "\n",
    "        # [seq_len,2]\n",
    "        cur_person_seq = cur_person_seq[:, 2:]\n",
    "        cur_person_seq_rel = np.zeros_like(cur_person_seq)\n",
    "\n",
    "        # first frame is zeros x,y\n",
    "        cur_person_seq_rel[1:, :] = cur_person_seq[1:, :] - \\\n",
    "            cur_person_seq[:-1, :]\n",
    "\n",
    "        cur_seq[count_person, :, :] = cur_person_seq\n",
    "        cur_seq_rel[count_person, :, :] = cur_person_seq_rel\n",
    "\n",
    "        frame_idxs = frames[idx:idx+seq_len]\n",
    "\n",
    "        # get the grid classification\n",
    "        if args.add_grid:\n",
    "          this_cur_person_seq = cur_person_seq\n",
    "          if args.traj_pixel is not None:  # use alternate xy\n",
    "            this_cur_person_seq = np.zeros_like(cur_person_seq)\n",
    "            for this_i, frame_idx in enumerate(frame_idxs):\n",
    "              key = \"%d_%d\" % (frame_idx, person_id)\n",
    "              # print key\n",
    "              x, y = args.traj_pixel[videoname][key]\n",
    "              this_cur_person_seq[this_i, :] = [x, y]\n",
    "\n",
    "          # get the grid classification label based on (x,y)\n",
    "          # grid centers: [H,W,2]\n",
    "          for i, (center, (h, w)) in enumerate(zip(\n",
    "              args.scene_grid_centers, args.scene_grids)):\n",
    "\n",
    "            # grid classification\n",
    "            h_gap, w_gap = args.video_h*1.0/h, args.video_w*1.0/w\n",
    "            x_indexes = np.ceil(this_cur_person_seq[:, 0] / w_gap)  # [seq_len]\n",
    "\n",
    "            y_indexes = np.ceil(this_cur_person_seq[:, 1] / h_gap)  # [seq_len]\n",
    "            x_indexes = np.asarray(x_indexes, dtype=\"int\")\n",
    "            y_indexes = np.asarray(y_indexes, dtype=\"int\")\n",
    "\n",
    "            # ceil(0.0) = 0.0, we need\n",
    "            x_indexes[x_indexes == 0] = 1\n",
    "            y_indexes[y_indexes == 0] = 1\n",
    "            x_indexes = x_indexes - 1\n",
    "            y_indexes = y_indexes - 1\n",
    "\n",
    "            one_hot = np.zeros((seq_len, h, w), dtype=\"uint8\")\n",
    "            one_hot[range(seq_len), y_indexes, x_indexes] = 1\n",
    "            one_hot_flat = one_hot.reshape((seq_len, -1))  # [seq_len,h*w]\n",
    "            classes = np.argmax(one_hot_flat, axis=1)  # [seq_len]\n",
    "            cur_seq_grids_class[count_person, i, :] = classes\n",
    "\n",
    "            # grid regression\n",
    "            # tile current person seq xy\n",
    "            cur_person_seq_tile = np.tile(np.expand_dims(np.expand_dims(\n",
    "                this_cur_person_seq, axis=1), axis=1), [1, h, w, 1])\n",
    "            # tile center [seq_len,h,w,2]\n",
    "            center_tile = np.tile(np.expand_dims(\n",
    "                center, axis=0), [seq_len, 1, 1, 1])\n",
    "            # grid_center + target -> actual xy\n",
    "            all_target = cur_person_seq_tile - center_tile  # [seq_len,h,w,2]\n",
    "            # only save the one grid\n",
    "            cur_seq_grids_target[count_person, i, :, :] = \\\n",
    "                all_target[one_hot.astype(\"bool\"), :]\n",
    "\n",
    "        # record the frame\n",
    "        cur_seq_frame[count_person, :] = frame_idxs\n",
    "\n",
    "        # kp feature\n",
    "        if args.add_kp:\n",
    "          # get the kp feature from starting frame to seq_len frame\n",
    "          for i, frame_idx in enumerate(frame_idxs):\n",
    "            key = \"%d_%d\" % (frame_idx, person_id)\n",
    "            # ignore the kp logits\n",
    "            kp_feat[count_person, i, :, :] = kp_feats[key][:, :2]\n",
    "\n",
    "          kp_feat_rel[count_person, 1:, :, :] = \\\n",
    "              kp_feat[count_person, 1:, :, :] - kp_feat[count_person, :-1, :, :]\n",
    "\n",
    "        if args.add_person_box:\n",
    "          for i, frame_idx in enumerate(frame_idxs):\n",
    "            key = \"%d_%d\" % (frame_idx, person_id)\n",
    "            person_box[count_person, i, :] = person_boxes[key]\n",
    "\n",
    "            # save this person key to an id\n",
    "            key = \"%s_%s\" % (videoname, key)\n",
    "\n",
    "            if key not in person_boxkey2id:\n",
    "              if args.person_boxkey2id is not None:\n",
    "                # use the boxid from previous preprocessed files\n",
    "                # to reproduce experiments\n",
    "                prev_boxid = args.person_boxkey2id[split][key]\n",
    "                person_boxkey2id[key] = prev_boxid\n",
    "                person_boxid2key[prev_boxid] = key\n",
    "              else:\n",
    "                new_person_boxid = len(person_boxkey2id)\n",
    "                person_boxkey2id[key] = new_person_boxid\n",
    "                person_boxid2key[new_person_boxid] = key\n",
    "\n",
    "            person_boxid = person_boxkey2id[key]\n",
    "            person_boxids[count_person, i] = person_boxid\n",
    "\n",
    "        if args.add_other_box:\n",
    "          this_other_box = []\n",
    "          this_other_box_class = []\n",
    "          for i, frame_idx in enumerate(frame_idxs):\n",
    "            key = \"%d_%d\" % (frame_idx, person_id)\n",
    "            # a list of [4]\n",
    "            this_other_box.append(other_boxes[key][0])\n",
    "            # a list of [1]\n",
    "            this_other_box_class.append(other_boxes[key][1])\n",
    "\n",
    "          other_box.append(this_other_box)\n",
    "          other_box_class.append(this_other_box_class)\n",
    "\n",
    "        if args.add_activity:\n",
    "          virat_timestep2fps = 12\n",
    "          this_cur_activity = []\n",
    "          this_future_activity = []\n",
    "          for i, frame_idx in enumerate(frame_idxs):\n",
    "            key = \"%d_%d\" % (frame_idx, person_id)\n",
    "            # a list of [1], act id;\n",
    "            # should not be empty, should have filled with BG class\n",
    "            current_actid_list, _, future_actid_list, _ = activities[key]\n",
    "            assert current_actid_list, current_actid_list\n",
    "            assert future_actid_list, future_actid_list\n",
    "\n",
    "            future_frame = int(args.pred_len * virat_timestep2fps)\n",
    "            future_actid_list_filtered = filter_future_act(\n",
    "                activities[key], future_frame)\n",
    "\n",
    "            # overlapping act?\n",
    "            current_actid_list = list(set(current_actid_list))\n",
    "            this_cur_activity.append(current_actid_list)\n",
    "\n",
    "            future_actid_list_filtered = list(set(future_actid_list_filtered))\n",
    "            this_future_activity.append(future_actid_list_filtered)\n",
    "\n",
    "          cur_activity.append(this_cur_activity)\n",
    "          future_activity.append(this_future_activity)\n",
    "\n",
    "        count_person += 1\n",
    "\n",
    "      # save the data\n",
    "      if count_person <= args.min_ped:\n",
    "        continue\n",
    "      num_person_in_start_frame.append(count_person)\n",
    "      # only count_person data is preserved\n",
    "      seq_list.append(cur_seq[:count_person])\n",
    "      seq_list_rel.append(cur_seq_rel[:count_person])\n",
    "\n",
    "      seq_frameidx_list.append(cur_seq_frame[:count_person])\n",
    "      seq_vid_list.append(cur_seq_vid[:count_person])\n",
    "\n",
    "      for one in cur_seq_frame[:count_person]:\n",
    "        for frameidx in one:\n",
    "          total_frame_used[(videoname, frameidx)] = 1\n",
    "\n",
    "      # other features\n",
    "      if args.add_kp:\n",
    "        kp_list.append(kp_feat[:count_person])\n",
    "        kp_list_rel.append(kp_feat_rel[:count_person])\n",
    "\n",
    "      if args.add_scene:\n",
    "        scene_list.append(scene_featidx[:count_person])\n",
    "\n",
    "      if args.add_grid:\n",
    "        seq_grid_class_list.append(cur_seq_grids_class[:count_person])\n",
    "        seq_grid_target_list.append(cur_seq_grids_target[:count_person])\n",
    "\n",
    "      if args.add_person_box:\n",
    "        box_seq_list.append(person_box[:count_person])\n",
    "        person_boxid_list.append(person_boxids[:count_person])\n",
    "\n",
    "      if args.add_other_box:\n",
    "        # other_box: [count_person, seqlen, K, 4] but python list,\n",
    "        # K is variable length\n",
    "        other_box_seq_list.extend(other_box)\n",
    "        other_box_class_seq_list.extend(other_box_class)\n",
    "\n",
    "      if args.add_activity:\n",
    "        # [count_person, seqlen, K] K is variable length\n",
    "        cur_act_list.extend(cur_activity)\n",
    "        future_act_list.extend(future_activity)\n",
    "\n",
    "  num_seq = len(seq_list)  # total number of frames across all videos\n",
    "  # [N*K, seq_len, 2]\n",
    "  # N is num_frame for each video, K is num_person in each frame\n",
    "  seq_list = np.concatenate(seq_list, axis=0)\n",
    "  seq_list_rel = np.concatenate(seq_list_rel, axis=0)\n",
    "  seq_frameidx_list = np.concatenate(seq_frameidx_list, axis=0)\n",
    "  seq_vid_list = np.concatenate(seq_vid_list, axis=0)\n",
    "\n",
    "  print(\"total frames %s, seq_list shape:%s, total unique frame used:%s\" %\n",
    "        (num_seq, seq_list.shape, len(total_frame_used)))\n",
    "\n",
    "  # we get the obs traj and pred_traj\n",
    "  # [N*K, obs_len, 2]\n",
    "  # [N*K, pred_len, 2]\n",
    "  obs_traj = seq_list[:, :obs_len, :]\n",
    "  pred_traj = seq_list[:, obs_len:, :]\n",
    "\n",
    "  obs_traj_rel = seq_list_rel[:, :obs_len, :]\n",
    "  pred_traj_rel = seq_list_rel[:, obs_len:, :]\n",
    "\n",
    "  # only save the obs_frames\n",
    "  obs_frameidx = seq_frameidx_list[:, :obs_len]\n",
    "  obs_vid = seq_vid_list[:]\n",
    "\n",
    "  # the starting idx for each frame in the N*K list,\n",
    "  # [num_frame, 2]\n",
    "  cum_start_idx = [0] + np.cumsum(num_person_in_start_frame).tolist()\n",
    "  seq_start_end = np.array([\n",
    "      (start, end) for start, end in zip(cum_start_idx, cum_start_idx[1:])\n",
    "  ], dtype=\"int\")\n",
    "\n",
    "  # save the data\n",
    "  data = {\n",
    "      \"obs_traj\": obs_traj,\n",
    "      \"obs_traj_rel\": obs_traj_rel,\n",
    "      \"pred_traj\": pred_traj,\n",
    "      \"pred_traj_rel\": pred_traj_rel,\n",
    "      \"seq_start_end\": seq_start_end,\n",
    "      \"obs_frameidx\": obs_frameidx,\n",
    "      \"obs_vid\": obs_vid,\n",
    "      \"vid2name\": vid2name,\n",
    "  }\n",
    "\n",
    "  if args.add_kp:\n",
    "    # [N*K, seq_len, 17, 2]\n",
    "    kp_list = np.concatenate(kp_list, axis=0)\n",
    "    kp_list_rel = np.concatenate(kp_list_rel, axis=0)\n",
    "\n",
    "    obs_kp = kp_list[:, :obs_len, :, :]\n",
    "    pred_kp = kp_list[:, obs_len:, :, :]  # for visualization\n",
    "    obs_kp_rel = kp_list_rel[:, :obs_len, :, :]\n",
    "\n",
    "    data.update({\n",
    "        \"obs_kp\": obs_kp,\n",
    "        \"obs_kp_rel\": obs_kp_rel,\n",
    "        \"pred_kp\": pred_kp,\n",
    "    })\n",
    "\n",
    "  if args.add_person_box:\n",
    "    box_seq_list = np.concatenate(box_seq_list, axis=0)\n",
    "\n",
    "    person_boxid_list = np.concatenate(\n",
    "        person_boxid_list, axis=0)  # [N,seq_len]\n",
    "\n",
    "    obs_box = box_seq_list[:, :obs_len, :]\n",
    "\n",
    "    obs_boxid = person_boxid_list[:, :obs_len]\n",
    "    data.update({\n",
    "        \"obs_box\": obs_box,\n",
    "        \"obs_boxid\": obs_boxid,\n",
    "        \"person_boxkey2id\": person_boxkey2id,\n",
    "        \"person_boxid2key\": person_boxid2key,\n",
    "    })\n",
    "    print(\"total unique person box:%s\" % len(person_boxid2key))\n",
    "\n",
    "  if args.add_other_box:\n",
    "    # other_box_seq_list a list  [N, count_person, seqlen, K, 4],\n",
    "    # K is variable length\n",
    "    other_box_seq_list = np.asarray(\n",
    "        other_box_seq_list)  # [N*K,seqlen] list type?\n",
    "    other_box_class_seq_list = np.asarray(\n",
    "        other_box_class_seq_list)  # [N*K,seqlen] list type?\n",
    "\n",
    "    data.update({\n",
    "        \"obs_other_box\": other_box_seq_list[:, :obs_len],\n",
    "        \"obs_other_box_class\": other_box_class_seq_list[:, :obs_len],\n",
    "    })\n",
    "\n",
    "  if args.add_activity:\n",
    "    cur_act_list = np.asarray(cur_act_list)\n",
    "    future_act_list = np.asarray(future_act_list)\n",
    "\n",
    "    # current/future activity *at* the last observed frame\n",
    "    cur_activity = cur_act_list[:, obs_len-1]\n",
    "    future_activity = future_act_list[:, obs_len-1]\n",
    "\n",
    "    # check some stats\n",
    "    cur_bg_only = []\n",
    "    cur_act_count = []\n",
    "    fu_act_count = []\n",
    "    fu_bg_only = []\n",
    "    for curs, fus in zip(cur_activity, future_activity):\n",
    "      cur_act_count.append(len(curs))\n",
    "      fu_act_count.append(len(fus))\n",
    "\n",
    "      first_cur = curs[0]\n",
    "      first_fu = fus[0]\n",
    "      if first_cur == 0:\n",
    "        assert len(curs) == 1, (curs, fus)\n",
    "        cur_bg_only.append(1)\n",
    "      else:\n",
    "        cur_bg_only.append(0)\n",
    "      if first_fu == 0:\n",
    "        assert len(fus) == 1, (curs, fus)\n",
    "        fu_bg_only.append(1)\n",
    "      else:\n",
    "        fu_bg_only.append(0)\n",
    "\n",
    "    move_cat = [\n",
    "        utils.activity2id[\"activity_walking\"],\n",
    "        utils.activity2id[\"activity_running\"],\n",
    "        utils.activity2id[\"Riding\"]\n",
    "    ]\n",
    "\n",
    "    traj_cat = np.zeros((len(cur_activity)), dtype=\"uint8\")\n",
    "    count_move = 0\n",
    "    for i in range(len(cur_activity)):\n",
    "      cur_acts = cur_activity[i]\n",
    "      move = False\n",
    "      for actid in cur_acts:\n",
    "        if actid in move_cat:\n",
    "          move = True\n",
    "          count_move += 1\n",
    "          break\n",
    "      traj_cat[i] = int(move)  # 0 -> static, 1 -> move\n",
    "\n",
    "    data.update({\n",
    "        \"cur_activity\": cur_activity,\n",
    "        \"future_activity\": future_activity,\n",
    "        \"traj_cat\": traj_cat  # 0, static, 1 is move\n",
    "    })\n",
    "\n",
    "  if args.add_grid:\n",
    "    seq_grid_class_list = np.concatenate(seq_grid_class_list, axis=0)\n",
    "    seq_grid_target_list = np.concatenate(seq_grid_target_list, axis=0)\n",
    "\n",
    "    obs_seq_grid_class = seq_grid_class_list[:, :, :obs_len]\n",
    "    obs_seq_grid_target = seq_grid_target_list[:, :, :obs_len]\n",
    "    pred_seq_grid_class = seq_grid_class_list[:, :, obs_len:]\n",
    "    pred_seq_grid_target = seq_grid_target_list[:, :, obs_len:]\n",
    "    data.update({\n",
    "        \"video_wh\": (args.video_w, args.video_h),\n",
    "        \"scene_grid_strides\": args.scene_grid_strides,\n",
    "        \"obs_grid_class\": obs_seq_grid_class,\n",
    "        \"obs_grid_target\": obs_seq_grid_target,\n",
    "        \"pred_grid_class\": pred_seq_grid_class,\n",
    "        \"pred_grid_target\": pred_seq_grid_target,\n",
    "    })\n",
    "    for i, center in enumerate(args.scene_grid_centers):\n",
    "      data.update({\n",
    "          (\"grid_center_%d\" % i): center,\n",
    "      })\n",
    "\n",
    "  if args.add_scene:\n",
    "    # the ids to the feature\n",
    "    # [N*K, seq_len, 1]\n",
    "    scene_list = np.concatenate(scene_list, axis=0)\n",
    "    obs_scene = scene_list[:, :obs_len, :]\n",
    "    pred_scene = scene_list[:, obs_len:, :]\n",
    "\n",
    "    # stack all the feature into one big matrix\n",
    "    # all frames in all videos # now it is jus the unique feature frame\n",
    "    total_frames = len(scene_feat_dict)\n",
    "    scene_feat_final_shape = (total_frames, scene_h,\n",
    "                              scene_w, total_scene_class)\n",
    "    # [6804, 288, 513, 41]\n",
    "    print(\"initilizing big scene feature matrix : %s..\" % list(\n",
    "        scene_feat_final_shape))\n",
    "    # each class will be a mask\n",
    "    scene_feat_final = np.zeros(scene_feat_final_shape, dtype=\"uint8\")\n",
    "    print(\"cool.\")\n",
    "    print(\"making mask scene feature...\")\n",
    "    for key in tqdm(scene_feat_dict, ascii=True):\n",
    "      feati = scene_key2feati[key]\n",
    "      scene_feat = scene_feat_dict[key]  # [H,W]\n",
    "      # transform classid first\n",
    "      new_scene_feat = np.zeros_like(scene_feat)  # zero for background class\n",
    "      for i in range(scene_h):\n",
    "        for j in range(scene_w):\n",
    "          # rest is ignored and all put into background\n",
    "          #if scene_oldid2new.has_key(scene_feat[i, j]):\n",
    "          if scene_feat[i, j] in scene_oldid2new:\n",
    "            new_scene_feat[i, j] = scene_oldid2new[scene_feat[i, j]]\n",
    "      # transform to masks\n",
    "      this_scene_feat = np.zeros(\n",
    "          (scene_h, scene_w, total_scene_class), dtype=\"uint8\")\n",
    "      # so we use the H,W to index the mask feat\n",
    "      # generate the index first\n",
    "      h_indexes = np.repeat(np.arange(scene_h), scene_w).reshape(\n",
    "          (scene_h, scene_w))\n",
    "      w_indexes = np.tile(np.arange(scene_w), scene_h).reshape(\n",
    "          (scene_h, scene_w))\n",
    "      this_scene_feat[h_indexes, w_indexes, new_scene_feat] = 1\n",
    "\n",
    "      scene_feat_final[feati, :, :, :] = this_scene_feat\n",
    "      del this_scene_feat\n",
    "      del new_scene_feat\n",
    "\n",
    "    data.update({\n",
    "        \"obs_scene\": obs_scene,\n",
    "        \"pred_scene\": pred_scene,\n",
    "        \"scene_feat\": scene_feat_final,\n",
    "    })\n",
    "\n",
    "  np.savez(prepro_path, **data)\n",
    "\n",
    "\n",
    "def filter_future_act(acts, future_frame):\n",
    "  \"\"\"Get future activity ids.\n",
    "  future activity from the data is all the future activity,\n",
    "  here we filter only the activity in pred_len,\n",
    "  also add the current activity that is still not finished\n",
    "  Args:\n",
    "    acts: a tuple of (current_actid_list, current_dist_list,\n",
    "        future_actid_list, future_dist_list)\n",
    "    future_frame: how many frame until the future\n",
    "  Returns:\n",
    "    future activity ids\n",
    "  \"\"\"\n",
    "\n",
    "  current_actid_list, current_dist_list, \\\n",
    "      future_actid_list, future_dist_list = acts\n",
    "\n",
    "  # leave the actid happens at future_frame\n",
    "  actids = []\n",
    "  for act_id, dist_to_finish in zip(current_actid_list, current_dist_list):\n",
    "    if act_id == 0:\n",
    "      continue\n",
    "    if future_frame <= dist_to_finish:\n",
    "      actids.append(act_id)\n",
    "\n",
    "  for act_id, dist_to_start in zip(future_actid_list, future_dist_list):\n",
    "    if act_id == 0:\n",
    "      continue\n",
    "    if future_frame >= dist_to_start:\n",
    "      actids.append(act_id)\n",
    "\n",
    "  if not actids:\n",
    "    actids.append(0)  # BG class\n",
    "\n",
    "  return actids\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  arguments = parser.parse_args()\n",
    "  main(arguments)\n",
    "© 2021 GitHub, Inc.\n",
    "Terms\n",
    "Privacy\n",
    "Security\n",
    "Status\n",
    "Docs\n",
    "Contact GitHub\n",
    "Pricing\n",
    "API\n",
    "Training\n",
    "Blog\n",
    "About\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
